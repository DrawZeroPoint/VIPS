\hypertarget{classVIPS__PythonWrapper}{}\section{V\+I\+P\+S\+\_\+\+Python\+Wrapper Class Reference}
\label{classVIPS__PythonWrapper}\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}


Collaboration diagram for V\+I\+P\+S\+\_\+\+Python\+Wrapper\+:\nopagebreak
\begin{figure}[H]
\begin{center}
\leavevmode
\includegraphics[width=350pt]{classVIPS__PythonWrapper__coll__graph}
\end{center}
\end{figure}
\subsection*{Public Member Functions}
\begin{DoxyCompactItemize}
\item 
{\bfseries V\+I\+P\+S\+\_\+\+Python\+Wrapper} (int num\+\_\+dimensions, int num\+\_\+threads, double min\+\_\+kl\+\_\+bound, double max\+\_\+kl\+\_\+bound)\hypertarget{classVIPS__PythonWrapper_ad5cc2281cd1d3abcd50618535c25b635}{}\label{classVIPS__PythonWrapper_ad5cc2281cd1d3abcd50618535c25b635}

\item 
void \hyperlink{classVIPS__PythonWrapper_a0a51f6e59a9b843e4a21262e4c119bbd}{add\+\_\+components} (double $\ast$new\+\_\+weights\+\_\+in, int new\+\_\+weights\+\_\+in\+\_\+dim1, double $\ast$new\+\_\+means\+\_\+in, int new\+\_\+means\+\_\+in\+\_\+dim1, int new\+\_\+means\+\_\+in\+\_\+dim2, double $\ast$new\+\_\+covs\+\_\+in, int new\+\_\+covs\+\_\+in\+\_\+dim1, int new\+\_\+covs\+\_\+in\+\_\+dim2, int new\+\_\+covs\+\_\+in\+\_\+dim3)
\item 
void \hyperlink{classVIPS__PythonWrapper_a8b17b5e96e7535358411ab8adcbcd934}{promote\+\_\+samples\+\_\+to\+\_\+components} (int N, double max\+\_\+exploration\+\_\+bonus, double tau, bool only\+\_\+check\+\_\+active\+\_\+samples, int max\+\_\+samples, bool scale\+\_\+entropy=true, bool isotropic=false)
\item 
void {\bfseries delete\+\_\+low\+\_\+weight\+\_\+components} (double min\+\_\+weight, int n\+\_\+del=300)\hypertarget{classVIPS__PythonWrapper_aeff812cf743741fcfce1871331ea14fb}{}\label{classVIPS__PythonWrapper_aeff812cf743741fcfce1871331ea14fb}

\item 
void \hyperlink{classVIPS__PythonWrapper_ac0ae21747614c8e406953e9151578053}{draw\+\_\+samples} (double N, double temperature, double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_adc5d4067954f4164c25426e032b5335d}{draw\+\_\+samples\+\_\+weights} (double N, double $\ast$new\+\_\+weights\+\_\+in, int new\+\_\+weights\+\_\+in\+\_\+dim1, double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a9c0a069ddf27394166c523834677165a}{add\+\_\+samples\+\_\+mean\+\_\+cov} (double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1, double $\ast$mean\+\_\+in, int mean\+\_\+in\+\_\+dim1, double $\ast$cov\+\_\+in, int cov\+\_\+in\+\_\+dim1, int cov\+\_\+in\+\_\+dim2)
\item 
void \hyperlink{classVIPS__PythonWrapper_a5a54eb8d14b32740b286f5f91660b6c2}{add\+\_\+samples} (double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, int $\ast$indices\+\_\+ptr, int indices\+\_\+dim1, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a1a2c8975cee0e96085040518a871184c}{activate\+\_\+newest\+\_\+samples} (int N, bool keep\+\_\+old)
\item 
void \hyperlink{classVIPS__PythonWrapper_ad93d42f4b18f28193aaba35129e7c068}{select\+\_\+active\+\_\+samples} (int num\+\_\+comps, int num\+\_\+samples, double temperature, double $\ast$$\ast$num\+\_\+eff\+\_\+out\+\_\+ptr, int $\ast$num\+\_\+eff\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a0e0bab32cbad5c82966440fd5d7e91db}{update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds} (bool update\+\_\+weight\+\_\+targets, bool update\+\_\+comp\+\_\+targets)
\item 
void \hyperlink{classVIPS__PythonWrapper_a79046b01fe1129abfd723c4d48ca1d45}{update\+\_\+weights} (double epsilon, double tau, double max\+\_\+entropy\+\_\+decrease, bool be\+\_\+greedy)
\item 
void \hyperlink{classVIPS__PythonWrapper_ae5a7f4052cffe00c4fee8950387af61d}{apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights} (double $\ast$lb\+\_\+weights\+\_\+in, int lb\+\_\+weights\+\_\+in\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a948b081338679c050ed174c6598383d3}{update\+\_\+components} (double tau, double ridge\+\_\+coefficient, double max\+\_\+entropy\+\_\+decrease, double max\+\_\+active\+\_\+samples, bool dont\+\_\+learn\+\_\+correlations, bool dont\+\_\+recompute\+\_\+densities, bool adapt\+\_\+ridge\+\_\+multipliers)
\item 
void \hyperlink{classVIPS__PythonWrapper_abf63652a9d7589cc189837ba41bcf322}{recompute\+\_\+densities} (bool only\+\_\+weights\+\_\+changed)
\item 
void \hyperlink{classVIPS__PythonWrapper_acb7c2dea0de7913cc96a8d9511cd96a8}{get\+\_\+model} (double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$means\+\_\+out\+\_\+ptr, int $\ast$means\+\_\+out\+\_\+dim1, int $\ast$means\+\_\+out\+\_\+dim2, double $\ast$$\ast$covs\+\_\+out\+\_\+ptr, int $\ast$covs\+\_\+out\+\_\+dim1, int $\ast$covs\+\_\+out\+\_\+dim2, int $\ast$covs\+\_\+out\+\_\+dim3)
\item 
void \hyperlink{classVIPS__PythonWrapper_acaee5a15878205a9ad0ce90c0ffbbdec}{get\+\_\+model\+\_\+entropies} (double $\ast$$\ast$entropies\+\_\+out\+\_\+ptr, int $\ast$entropies\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a2cf72db0bbb7259955d9229ab768cf6b}{get\+\_\+background} (double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$means\+\_\+out\+\_\+ptr, int $\ast$means\+\_\+out\+\_\+dim1, int $\ast$means\+\_\+out\+\_\+dim2, double $\ast$$\ast$covs\+\_\+out\+\_\+ptr, int $\ast$covs\+\_\+out\+\_\+dim1, int $\ast$covs\+\_\+out\+\_\+dim2, int $\ast$covs\+\_\+out\+\_\+dim3)
\item 
void \hyperlink{classVIPS__PythonWrapper_aa48468dd40833596d32e202fade3af84}{get\+\_\+last\+\_\+\+K\+Ls} (double $\ast$K\+L\+\_\+weights\+\_\+out, double $\ast$$\ast$kls\+\_\+comp\+\_\+out, int $\ast$kls\+\_\+comp\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a6533e3114818b6dc596a51bb8d8aae32}{get\+\_\+weights} (double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+weights\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+weights\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_aa650a92bb89042882d548a16fdf46205}{get\+\_\+num\+\_\+samples} (int $\ast$num\+\_\+samples, int $\ast$num\+\_\+samples\+\_\+total)
\item 
void \hyperlink{classVIPS__PythonWrapper_acafd12e5bc30f51b703a73606d76de40}{get\+\_\+expected\+\_\+rewards} (double $\ast$$\ast$expected\+\_\+rewards\+\_\+out, int $\ast$expected\+\_\+rewards\+\_\+out\+\_\+dim1, double $\ast$$\ast$expected\+\_\+target\+\_\+densities\+\_\+out, int $\ast$expected\+\_\+target\+\_\+densities\+\_\+out\+\_\+dim1, double $\ast$$\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim2, double $\ast$$\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim2)
\item 
void \hyperlink{classVIPS__PythonWrapper_a84f192f13f7e83da0fff4801121a49c3}{get\+\_\+best\+\_\+interpolation} (int index, double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1, double $\ast$target\+\_\+densities2\+\_\+ptr, int td2\+\_\+dim1, double scaling\+\_\+factor)
\item 
void \hyperlink{classVIPS__PythonWrapper_a4cd577f2c0ae57ba85fff0746f7c7d7a}{get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture} (double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$$\ast$sample\+\_\+densities\+\_\+out, int $\ast$sample\+\_\+densities\+\_\+out\+\_\+dim1)
\item 
void \hyperlink{classVIPS__PythonWrapper_a2be7ce5aaa742d979b35ade922139894}{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples} (double $\ast$entropy)
\item 
void \hyperlink{classVIPS__PythonWrapper_a8065f48c6060b03eaa2b5c7f1f89c636}{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples} (double $\ast$entropy, int num\+\_\+samples=10000)
\item 
void \hyperlink{classVIPS__PythonWrapper_aeef466a77710252e595fcd6e9909f975}{get\+\_\+num\+\_\+components} (int $\ast$num\+\_\+components\+\_\+out)
\item 
void \hyperlink{classVIPS__PythonWrapper_a64a647b24326d512082dbb60932eaa29}{get\+\_\+debug\+\_\+info} (double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, double $\ast$$\ast$target\+\_\+densities\+\_\+out\+\_\+ptr, int $\ast$target\+\_\+densities\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+responsibilities\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+responsibilities\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+responsibilities\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+dim1, double $\ast$$\ast$importance\+\_\+weights\+\_\+out\+\_\+ptr, int $\ast$importance\+\_\+weights\+\_\+out\+\_\+dim1, int $\ast$importance\+\_\+weights\+\_\+out\+\_\+dim2, double $\ast$$\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+ptr, int $\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim1, int $\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1, int $\ast$$\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim2)
\item 
void \hyperlink{classVIPS__PythonWrapper_a8d258a474d63f442b89358c1793ccf7e}{compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components} (bool reverse\+\_\+\+KL, float $\ast$$\ast$K\+L\+\_\+mat\+\_\+out\+\_\+ptr, int $\ast$K\+L\+\_\+mat\+\_\+out\+\_\+dim1, int $\ast$K\+L\+\_\+mat\+\_\+out\+\_\+dim2)
\item 
void \hyperlink{classVIPS__PythonWrapper_a007140eac0ea0ea69de16a7eb8c13293}{backup\+\_\+learner} ()
\item 
void \hyperlink{classVIPS__PythonWrapper_a3819321597ef3f522bd577f479ff81b0}{restore\+\_\+learner} ()
\end{DoxyCompactItemize}
\subsection*{Protected Attributes}
\begin{DoxyCompactItemize}
\item 
\hyperlink{classVIPS}{V\+I\+PS} {\bfseries learner}\hypertarget{classVIPS__PythonWrapper_a967a73404906831a828f926b4fc8c7b2}{}\label{classVIPS__PythonWrapper_a967a73404906831a828f926b4fc8c7b2}

\item 
\hyperlink{classVIPS}{V\+I\+PS} {\bfseries learner\+\_\+backup}\hypertarget{classVIPS__PythonWrapper_aaa74d7a0f9b7039f49cd7736f0fd463a}{}\label{classVIPS__PythonWrapper_aaa74d7a0f9b7039f49cd7736f0fd463a}

\item 
int {\bfseries num\+\_\+dimensions}\hypertarget{classVIPS__PythonWrapper_ae70db7c8f890fe7bba73c15de92e9d0b}{}\label{classVIPS__PythonWrapper_ae70db7c8f890fe7bba73c15de92e9d0b}

\item 
arma\+::vec {\bfseries last\+\_\+\+K\+Ls\+\_\+components}\hypertarget{classVIPS__PythonWrapper_a0d7e2c9449dc4411ba71d623c8a4d236}{}\label{classVIPS__PythonWrapper_a0d7e2c9449dc4411ba71d623c8a4d236}

\item 
double {\bfseries last\+\_\+\+K\+L\+\_\+weights}\hypertarget{classVIPS__PythonWrapper_aa37f34c104c8b41f3d8fd7a1023da696}{}\label{classVIPS__PythonWrapper_aa37f34c104c8b41f3d8fd7a1023da696}

\item 
arma\+::vec {\bfseries last\+\_\+expected\+\_\+rew}\hypertarget{classVIPS__PythonWrapper_ad33747fdb016a926ab1ea3524aa2b36b}{}\label{classVIPS__PythonWrapper_ad33747fdb016a926ab1ea3524aa2b36b}

\item 
arma\+::vec {\bfseries last\+\_\+expected\+\_\+target\+\_\+densities}\hypertarget{classVIPS__PythonWrapper_afd270b0737302fb8a3e755057f4363c2}{}\label{classVIPS__PythonWrapper_afd270b0737302fb8a3e755057f4363c2}

\end{DoxyCompactItemize}


\subsection{Member Function Documentation}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!activate\+\_\+newest\+\_\+samples@{activate\+\_\+newest\+\_\+samples}}
\index{activate\+\_\+newest\+\_\+samples@{activate\+\_\+newest\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{activate\+\_\+newest\+\_\+samples(int N, bool keep\+\_\+old)}{activate_newest_samples(int N, bool keep_old)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::activate\+\_\+newest\+\_\+samples (
\begin{DoxyParamCaption}
\item[{int}]{N, }
\item[{bool}]{keep\+\_\+old}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a1a2c8975cee0e96085040518a871184c}{}\label{classVIPS__PythonWrapper_a1a2c8975cee0e96085040518a871184c}
Selects the N most recent samples and activates them (i.\+e. uses them for the upcoming learning iteration).~\newline
 Makes sure, that all relevant data (e.\+g. densities, importance weights, etc.) gets updated 
\begin{DoxyParams}{Parameters}
{\em N} & -\/ the maximum number of recent samples to be activated, actually number of activated samples might be less, iff the sample database does not contain sufficient samples.~\newline
 \\
\hline
{\em keep\+\_\+old} & -\/ iff set to true, the currently active sample will remain active \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!add\+\_\+components@{add\+\_\+components}}
\index{add\+\_\+components@{add\+\_\+components}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{add\+\_\+components(double $\ast$new\+\_\+weights\+\_\+in, int new\+\_\+weights\+\_\+in\+\_\+dim1, double $\ast$new\+\_\+means\+\_\+in, int new\+\_\+means\+\_\+in\+\_\+dim1, int new\+\_\+means\+\_\+in\+\_\+dim2, double $\ast$new\+\_\+covs\+\_\+in, int new\+\_\+covs\+\_\+in\+\_\+dim1, int new\+\_\+covs\+\_\+in\+\_\+dim2, int new\+\_\+covs\+\_\+in\+\_\+dim3)}{add_components(double *new_weights_in, int new_weights_in_dim1, double *new_means_in, int new_means_in_dim1, int new_means_in_dim2, double *new_covs_in, int new_covs_in_dim1, int new_covs_in_dim2, int new_covs_in_dim3)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::add\+\_\+components (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{new\+\_\+weights\+\_\+in, }
\item[{int}]{new\+\_\+weights\+\_\+in\+\_\+dim1, }
\item[{double $\ast$}]{new\+\_\+means\+\_\+in, }
\item[{int}]{new\+\_\+means\+\_\+in\+\_\+dim1, }
\item[{int}]{new\+\_\+means\+\_\+in\+\_\+dim2, }
\item[{double $\ast$}]{new\+\_\+covs\+\_\+in, }
\item[{int}]{new\+\_\+covs\+\_\+in\+\_\+dim1, }
\item[{int}]{new\+\_\+covs\+\_\+in\+\_\+dim2, }
\item[{int}]{new\+\_\+covs\+\_\+in\+\_\+dim3}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a0a51f6e59a9b843e4a21262e4c119bbd}{}\label{classVIPS__PythonWrapper_a0a51f6e59a9b843e4a21262e4c119bbd}
Adds new components~\newline
. 
\begin{DoxyParams}{Parameters}
{\em new\+\_\+weights} & -\/ new weights of the \hyperlink{classGMM}{G\+MM} (including existing components) \\
\hline
{\em new\+\_\+means} & -\/ matrix of size N\+\_\+dimensions x N\+\_\+new\+Components specifying the means of the new components \\
\hline
{\em new\+\_\+covs} & -\/ cube of size N\+\_\+dimensions x N\+\_\+dimensions x N\+\_\+new\+Components specifying the covariance matrices of the new components \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!add\+\_\+samples@{add\+\_\+samples}}
\index{add\+\_\+samples@{add\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{add\+\_\+samples(double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, int $\ast$indices\+\_\+ptr, int indices\+\_\+dim1, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1)}{add_samples(double *samples_ptr, int samples_dim1, int samples_dim2, int *indices_ptr, int indices_dim1, double *target_densities_ptr, int td_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::add\+\_\+samples (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{samples\+\_\+ptr, }
\item[{int}]{samples\+\_\+dim1, }
\item[{int}]{samples\+\_\+dim2, }
\item[{int $\ast$}]{indices\+\_\+ptr, }
\item[{int}]{indices\+\_\+dim1, }
\item[{double $\ast$}]{target\+\_\+densities\+\_\+ptr, }
\item[{int}]{td\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a5a54eb8d14b32740b286f5f91660b6c2}{}\label{classVIPS__PythonWrapper_a5a54eb8d14b32740b286f5f91660b6c2}
Adds new samples to the database. Note that the samples will not be used for learning, until they have been activated (see activate\+\_\+newest\+\_\+samples).~\newline
 The samples are assumed to have been drawn from the current model and the indices of the relevant components are to be provided for computing background distributions when necessary. 
\begin{DoxyParams}{Parameters}
{\em new\+\_\+samples} & -\/ a matrix of size N\+\_\+dimensions X N\+\_\+samples \\
\hline
{\em used\+\_\+components} & -\/ a vector of size N\+\_\+samples containing the indices of the components the corresponding samples have been drawn from \\
\hline
{\em new\+\_\+target\+\_\+densities} & -\/ a vector of size N\+\_\+samples containing the unnormalized densities on the target distribution \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
\hyperlink{classVIPS__PythonWrapper_a1a2c8975cee0e96085040518a871184c}{activate\+\_\+newest\+\_\+samples} 
\end{DoxySeeAlso}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!add\+\_\+samples\+\_\+mean\+\_\+cov@{add\+\_\+samples\+\_\+mean\+\_\+cov}}
\index{add\+\_\+samples\+\_\+mean\+\_\+cov@{add\+\_\+samples\+\_\+mean\+\_\+cov}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{add\+\_\+samples\+\_\+mean\+\_\+cov(double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1, double $\ast$mean\+\_\+in, int mean\+\_\+in\+\_\+dim1, double $\ast$cov\+\_\+in, int cov\+\_\+in\+\_\+dim1, int cov\+\_\+in\+\_\+dim2)}{add_samples_mean_cov(double *samples_ptr, int samples_dim1, int samples_dim2, double *target_densities_ptr, int td_dim1, double *mean_in, int mean_in_dim1, double *cov_in, int cov_in_dim1, int cov_in_dim2)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::add\+\_\+samples\+\_\+mean\+\_\+cov (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{samples\+\_\+ptr, }
\item[{int}]{samples\+\_\+dim1, }
\item[{int}]{samples\+\_\+dim2, }
\item[{double $\ast$}]{target\+\_\+densities\+\_\+ptr, }
\item[{int}]{td\+\_\+dim1, }
\item[{double $\ast$}]{mean\+\_\+in, }
\item[{int}]{mean\+\_\+in\+\_\+dim1, }
\item[{double $\ast$}]{cov\+\_\+in, }
\item[{int}]{cov\+\_\+in\+\_\+dim1, }
\item[{int}]{cov\+\_\+in\+\_\+dim2}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a9c0a069ddf27394166c523834677165a}{}\label{classVIPS__PythonWrapper_a9c0a069ddf27394166c523834677165a}
Adds new samples to the database. Note that the samples will not be used for learning, until they have been activated (see activate\+\_\+newest\+\_\+samples).~\newline
 The samples are assumed to have been drawn from a Gaussian with specified mean and covariance matrix 
\begin{DoxyParams}{Parameters}
{\em new\+\_\+samples} & -\/ a matrix of size N\+\_\+dimensions X N\+\_\+samples \\
\hline
{\em new\+\_\+target\+\_\+densities} & -\/ a vector of size N\+\_\+samples containing the unnormalized densities on the target distribution \\
\hline
{\em used\+\_\+mean} & -\/ a vector containing the mean of the Gaussian used for drawing the samples \\
\hline
{\em used\+\_\+cov} & -\/ a matrix containing the covariance matrix of the Gaussian used for drawing the samples \\
\hline
\end{DoxyParams}
\begin{DoxySeeAlso}{See also}
\hyperlink{classVIPS__PythonWrapper_a1a2c8975cee0e96085040518a871184c}{activate\+\_\+newest\+\_\+samples} 
\end{DoxySeeAlso}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights@{apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights}}
\index{apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights@{apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights(double $\ast$lb\+\_\+weights\+\_\+in, int lb\+\_\+weights\+\_\+in\+\_\+dim1)}{apply_lower_bound_on_weights(double *lb_weights_in, int lb_weights_in_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::apply\+\_\+lower\+\_\+bound\+\_\+on\+\_\+weights (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{lb\+\_\+weights\+\_\+in, }
\item[{int}]{lb\+\_\+weights\+\_\+in\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_ae5a7f4052cffe00c4fee8950387af61d}{}\label{classVIPS__PythonWrapper_ae5a7f4052cffe00c4fee8950387af61d}
Lower bounds all weights and renormalizes by scaling those weights that have not been modified. Currently, we do not check whether weights that got rescaled drop below their lower bound. 
\begin{DoxyParams}{Parameters}
{\em lb\+\_\+weights\+\_\+in} & -\/ vector containing the lower bound for all weights. The sum of lb\+\_\+weights\+\_\+in needs to be $<$= 1 \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!backup\+\_\+learner@{backup\+\_\+learner}}
\index{backup\+\_\+learner@{backup\+\_\+learner}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{backup\+\_\+learner()}{backup_learner()}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::backup\+\_\+learner (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a007140eac0ea0ea69de16a7eb8c13293}{}\label{classVIPS__PythonWrapper_a007140eac0ea0ea69de16a7eb8c13293}
creates a backup of the the current state of the learner (overwriting previous backup). \begin{DoxySeeAlso}{See also}
restore\+\_\+backup() 
\end{DoxySeeAlso}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components@{compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components}}
\index{compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components@{compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components(bool reverse\+\_\+\+K\+L, float $\ast$$\ast$\+K\+L\+\_\+mat\+\_\+out\+\_\+ptr, int $\ast$\+K\+L\+\_\+mat\+\_\+out\+\_\+dim1, int $\ast$\+K\+L\+\_\+mat\+\_\+out\+\_\+dim2)}{compute_KLs_between_GMM_and_DB_components(bool reverse_KL, float **KL_mat_out_ptr, int *KL_mat_out_dim1, int *KL_mat_out_dim2)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::compute\+\_\+\+K\+Ls\+\_\+between\+\_\+\+G\+M\+M\+\_\+and\+\_\+\+D\+B\+\_\+components (
\begin{DoxyParamCaption}
\item[{bool}]{reverse\+\_\+\+KL, }
\item[{float $\ast$$\ast$}]{K\+L\+\_\+mat\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{K\+L\+\_\+mat\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{K\+L\+\_\+mat\+\_\+out\+\_\+dim2}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a8d258a474d63f442b89358c1793ccf7e}{}\label{classVIPS__PythonWrapper_a8d258a474d63f442b89358c1793ccf7e}
For each component of the current approximation compute the KL w.\+r.\+t. each component that is stored in the database. If the parameter reverse\+\_\+\+KL is set to true, computes instead the KL for each database component w.\+r.\+t. each \hyperlink{classGMM}{G\+MM} component 
\begin{DoxyParams}{Parameters}
{\em reverse\+\_\+\+KL} & -\/ if set to bool compute the KL between database\+\_\+componets w.\+r.\+t. \hyperlink{classGMM}{G\+MM} components \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
a matrix of size components\+\_\+in\+\_\+gmm x components\+\_\+in\+\_\+database (or transposed if computing the reverse KL) containing the relative entropies. 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!draw\+\_\+samples@{draw\+\_\+samples}}
\index{draw\+\_\+samples@{draw\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{draw\+\_\+samples(double N, double temperature, double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1)}{draw_samples(double N, double temperature, double **samples_out_ptr, int *samples_out_dim1, int *samples_out_dim2, int **indices_out, int *indices_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::draw\+\_\+samples (
\begin{DoxyParamCaption}
\item[{double}]{N, }
\item[{double}]{temperature, }
\item[{double $\ast$$\ast$}]{samples\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim2, }
\item[{int $\ast$$\ast$}]{indices\+\_\+out, }
\item[{int $\ast$}]{indices\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_ac0ae21747614c8e406953e9151578053}{}\label{classVIPS__PythonWrapper_ac0ae21747614c8e406953e9151578053}
Sample from the current \hyperlink{classGMM}{G\+MM} approximation, but use the specified temperature to to adapt the weights, i.\+e. the coefficient are replaced by p\textquotesingle{}(o)  exp(log(p(o)$\ast$temperature)) 
\begin{DoxyParams}{Parameters}
{\em N} & -\/ the numb er of samples to been drawn \\
\hline
{\em weights} & -\/ the weights (coefficients) to be used \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
samples -\/ N Samples drwan from the mixture with the given weights 

indices -\/ for each sample, the index of the component that was used for drawing it 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!draw\+\_\+samples\+\_\+weights@{draw\+\_\+samples\+\_\+weights}}
\index{draw\+\_\+samples\+\_\+weights@{draw\+\_\+samples\+\_\+weights}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{draw\+\_\+samples\+\_\+weights(double N, double $\ast$new\+\_\+weights\+\_\+in, int new\+\_\+weights\+\_\+in\+\_\+dim1, double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1)}{draw_samples_weights(double N, double *new_weights_in, int new_weights_in_dim1, double **samples_out_ptr, int *samples_out_dim1, int *samples_out_dim2, int **indices_out, int *indices_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::draw\+\_\+samples\+\_\+weights (
\begin{DoxyParamCaption}
\item[{double}]{N, }
\item[{double $\ast$}]{new\+\_\+weights\+\_\+in, }
\item[{int}]{new\+\_\+weights\+\_\+in\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{samples\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim2, }
\item[{int $\ast$$\ast$}]{indices\+\_\+out, }
\item[{int $\ast$}]{indices\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_adc5d4067954f4164c25426e032b5335d}{}\label{classVIPS__PythonWrapper_adc5d4067954f4164c25426e032b5335d}
Sample from the current \hyperlink{classGMM}{G\+MM} approximation, but use the specified component coefficients. 
\begin{DoxyParams}{Parameters}
{\em N} & -\/ the numb er of samples to been drawn \\
\hline
{\em weights} & -\/ the weights (coefficients) to be used \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
samples -\/ N Samples drwan from the mixture with the given weights 

indices -\/ for each sample, the index of the component that was used for drawing it 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+background@{get\+\_\+background}}
\index{get\+\_\+background@{get\+\_\+background}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+background(double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$means\+\_\+out\+\_\+ptr, int $\ast$means\+\_\+out\+\_\+dim1, int $\ast$means\+\_\+out\+\_\+dim2, double $\ast$$\ast$covs\+\_\+out\+\_\+ptr, int $\ast$covs\+\_\+out\+\_\+dim1, int $\ast$covs\+\_\+out\+\_\+dim2, int $\ast$covs\+\_\+out\+\_\+dim3)}{get_background(double **weights_out_ptr, int *weights_out_dim1, double **means_out_ptr, int *means_out_dim1, int *means_out_dim2, double **covs_out_ptr, int *covs_out_dim1, int *covs_out_dim2, int *covs_out_dim3)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+background (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{weights\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{weights\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{means\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{means\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{means\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{covs\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim2, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim3}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a2cf72db0bbb7259955d9229ab768cf6b}{}\label{classVIPS__PythonWrapper_a2cf72db0bbb7259955d9229ab768cf6b}
Returns the background distribution used for computing importance weights. \begin{DoxyReturn}{Returns}
weights -\/ weights for each component 

means -\/ means for each component 

covs -\/ covariance matrices for each component 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+best\+\_\+interpolation@{get\+\_\+best\+\_\+interpolation}}
\index{get\+\_\+best\+\_\+interpolation@{get\+\_\+best\+\_\+interpolation}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+best\+\_\+interpolation(int index, double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$target\+\_\+densities\+\_\+ptr, int td\+\_\+dim1, double $\ast$target\+\_\+densities2\+\_\+ptr, int td2\+\_\+dim1, double scaling\+\_\+factor)}{get_best_interpolation(int index, double *samples_ptr, int samples_dim1, int samples_dim2, double *target_densities_ptr, int td_dim1, double *target_densities2_ptr, int td2_dim1, double scaling_factor)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+best\+\_\+interpolation (
\begin{DoxyParamCaption}
\item[{int}]{index, }
\item[{double $\ast$}]{samples\+\_\+ptr, }
\item[{int}]{samples\+\_\+dim1, }
\item[{int}]{samples\+\_\+dim2, }
\item[{double $\ast$}]{target\+\_\+densities\+\_\+ptr, }
\item[{int}]{td\+\_\+dim1, }
\item[{double $\ast$}]{target\+\_\+densities2\+\_\+ptr, }
\item[{int}]{td2\+\_\+dim1, }
\item[{double}]{scaling\+\_\+factor}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a84f192f13f7e83da0fff4801121a49c3}{}\label{classVIPS__PythonWrapper_a84f192f13f7e83da0fff4801121a49c3}
For a given component, find the best interpolation between the current covariance matrix and a spherical one using a linear linesearch. ~\newline
 The quality of the covariance matrix is estimated based on an importance weighted Monte Carlo estimate of the expected loglikelihood on the target distribution. ~\newline
 The Monte Carlo estimate is computed based on a given set of samples, the corresponding target loglikelihoods and the loglikelihood of the sampling distribution. ~\newline
 I\+M\+P\+O\+R\+T\+A\+NT\+: This method will change the component to the best interpolation. ~\newline
 
\begin{DoxyParams}{Parameters}
{\em index} & -\/ index specifying which component of the current model should be adapted ~\newline
 \\
\hline
{\em samples\+\_\+ptr} & -\/ samples for the Monte Carlo estimate ~\newline
 \\
\hline
{\em target\+\_\+densities\+\_\+ptr} & -\/ vector containing the loglikelihood under the target distribution for each sample ~\newline
 \\
\hline
{\em target\+\_\+densities2\+\_\+ptr} & -\/ vector containing the loglikelihood of the distribution that was used for drawing the samples ~\newline
 \\
\hline
{\em scaling\+\_\+factor} & -\/ the spherical covariance matrix is given by scaling\+\_\+factor $\ast$ eye() ~\newline
 \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+debug\+\_\+info@{get\+\_\+debug\+\_\+info}}
\index{get\+\_\+debug\+\_\+info@{get\+\_\+debug\+\_\+info}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+debug\+\_\+info(double $\ast$$\ast$samples\+\_\+out\+\_\+ptr, int $\ast$samples\+\_\+out\+\_\+dim1, int $\ast$samples\+\_\+out\+\_\+dim2, double $\ast$$\ast$target\+\_\+densities\+\_\+out\+\_\+ptr, int $\ast$target\+\_\+densities\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+responsibilities\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+responsibilities\+\_\+out\+\_\+dim1, int $\ast$log\+\_\+responsibilities\+\_\+out\+\_\+dim2, double $\ast$$\ast$log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+dim1, double $\ast$$\ast$importance\+\_\+weights\+\_\+out\+\_\+ptr, int $\ast$importance\+\_\+weights\+\_\+out\+\_\+dim1, int $\ast$importance\+\_\+weights\+\_\+out\+\_\+dim2, double $\ast$$\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+ptr, int $\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim1, int $\ast$importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim2, int $\ast$$\ast$indices\+\_\+out, int $\ast$indices\+\_\+out\+\_\+dim1, int $\ast$$\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim2)}{get_debug_info(double **samples_out_ptr, int *samples_out_dim1, int *samples_out_dim2, double **target_densities_out_ptr, int *target_densities_out_dim1, double **log_densities_on_model_comps_out_ptr, int *log_densities_on_model_comps_out_dim1, int *log_densities_on_model_comps_out_dim2, double **log_joint_densities_out_ptr, int *log_joint_densities_out_dim1, int *log_joint_densities_out_dim2, double **log_densities_on_model_out_ptr, int *log_densities_on_model_out_dim1, double **log_responsibilities_out_ptr, int *log_responsibilities_out_dim1, int *log_responsibilities_out_dim2, double **log_densities_on_background_out_ptr, int *log_densities_on_background_out_dim1, double **importance_weights_out_ptr, int *importance_weights_out_dim1, int *importance_weights_out_dim2, double **importance_weights_normalized_out_ptr, int *importance_weights_normalized_out_dim1, int *importance_weights_normalized_out_dim2, int **indices_out, int *indices_out_dim1, int **num_samples_history_out_ptr, int *num_samples_history_out_dim1, int *num_samples_history_out_dim2)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+debug\+\_\+info (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{samples\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{samples\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{target\+\_\+densities\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{target\+\_\+densities\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{log\+\_\+densities\+\_\+on\+\_\+model\+\_\+comps\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{log\+\_\+joint\+\_\+densities\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+densities\+\_\+on\+\_\+model\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{log\+\_\+responsibilities\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+responsibilities\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{log\+\_\+responsibilities\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+densities\+\_\+on\+\_\+background\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{importance\+\_\+weights\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{importance\+\_\+weights\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{importance\+\_\+weights\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{importance\+\_\+weights\+\_\+normalized\+\_\+out\+\_\+dim2, }
\item[{int $\ast$$\ast$}]{indices\+\_\+out, }
\item[{int $\ast$}]{indices\+\_\+out\+\_\+dim1, }
\item[{int $\ast$$\ast$}]{num\+\_\+samples\+\_\+history\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{num\+\_\+samples\+\_\+history\+\_\+out\+\_\+dim2}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a64a647b24326d512082dbb60932eaa29}{}\label{classVIPS__PythonWrapper_a64a647b24326d512082dbb60932eaa29}
Get various densities, importance weights, etc. for debug purposes. \begin{DoxyReturn}{Returns}
a tuple, st. ~\newline
 tuple\mbox{[}0\mbox{]} contains the samples ~\newline
 tuple\mbox{[}1\mbox{]} contains the unnormalized target densities ~\newline
 tuple\mbox{[}2\mbox{]} contains the log densities on each model p(s$\vert$o) ~\newline
 tuple\mbox{[}3\mbox{]} contains the joint log densities p(s,o) ~\newline
 tuple\mbox{[}4\mbox{]} contains the \hyperlink{classGMM}{G\+MM} densities p(s) ~\newline
 tuple\mbox{[}5\mbox{]} contains the log responsibilities p(o$\vert$s) ~\newline
 tuple\mbox{[}6\mbox{]} contains the densitis on the background distribution q(s) ~\newline
 tuple\mbox{[}7\mbox{]} contains the importance weights ~\newline
 tuple\mbox{[}8\mbox{]} contains the normalized importance weights ~\newline
 tuple\mbox{[}9\mbox{]} contains the indices of the active samples ~\newline
 tuple\mbox{[}10\mbox{]} contains the history of the number of samples drawn from each component 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples@{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples}}
\index{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples@{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples(double $\ast$entropy)}{get_entropy_estimate_on_active_samples(double *entropy)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{entropy}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a2be7ce5aaa742d979b35ade922139894}{}\label{classVIPS__PythonWrapper_a2be7ce5aaa742d979b35ade922139894}
Returns a (weighted importance sampling) Monte-\/\+Carlo estimate of the entropy of the learned model. The entropy is computed based on the active samples. As this entropy is computed based on precomputed values \mbox{[}importance weights and log(p(x))\mbox{]} th evaluation is very fast. However, if the number of active samples is still low or the samples are not \char`\"{}fresh\char`\"{} (low importance weights), the estimate can be quite bad. \begin{DoxySeeAlso}{See also}
\hyperlink{classVIPS__PythonWrapper_a8065f48c6060b03eaa2b5c7f1f89c636}{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples()} for a slower, but usually more accurate estimate. 
\end{DoxySeeAlso}
\begin{DoxyReturn}{Returns}
a Monte-\/\+Carlo estimate of the entropy of the learned Gaussian Mixture Model 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples@{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples}}
\index{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples@{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples(double $\ast$entropy, int num\+\_\+samples=10000)}{get_entropy_estimate_on_gmm_samples(double *entropy, int num_samples=10000)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+gmm\+\_\+samples (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{entropy, }
\item[{int}]{num\+\_\+samples = {\ttfamily 10000}}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a8065f48c6060b03eaa2b5c7f1f89c636}{}\label{classVIPS__PythonWrapper_a8065f48c6060b03eaa2b5c7f1f89c636}
Returns a Monte-\/\+Carlo estimate of the entropy of the learned model. This methods draws new samples from the learned model and evaluated their log-\/densities log(p(x)) The entropy of the learned model is then approximated as H(p)  -\/1/num\+\_\+samples  log(p(x)) \begin{DoxySeeAlso}{See also}
\hyperlink{classVIPS__PythonWrapper_a2be7ce5aaa742d979b35ade922139894}{get\+\_\+entropy\+\_\+estimate\+\_\+on\+\_\+active\+\_\+samples()} for a faster, but usually less accurate estimate. 
\end{DoxySeeAlso}

\begin{DoxyParams}{Parameters}
{\em num\+\_\+samples} & -\/ the number of samples that should be drawn for computing the estimate \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
a Monte-\/\+Carlo estimate of the entropy of the learned Gaussian Mixture Model 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+expected\+\_\+rewards@{get\+\_\+expected\+\_\+rewards}}
\index{get\+\_\+expected\+\_\+rewards@{get\+\_\+expected\+\_\+rewards}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+expected\+\_\+rewards(double $\ast$$\ast$expected\+\_\+rewards\+\_\+out, int $\ast$expected\+\_\+rewards\+\_\+out\+\_\+dim1, double $\ast$$\ast$expected\+\_\+target\+\_\+densities\+\_\+out, int $\ast$expected\+\_\+target\+\_\+densities\+\_\+out\+\_\+dim1, double $\ast$$\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim2, double $\ast$$\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+ptr, int $\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim1, int $\ast$comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim2)}{get_expected_rewards(double **expected_rewards_out, int *expected_rewards_out_dim1, double **expected_target_densities_out, int *expected_target_densities_out_dim1, double **comp_etd_history_out_ptr, int *comp_etd_history_out_dim1, int *comp_etd_history_out_dim2, double **comp_reward_history_out_ptr, int *comp_reward_history_out_dim1, int *comp_reward_history_out_dim2)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+expected\+\_\+rewards (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{expected\+\_\+rewards\+\_\+out, }
\item[{int $\ast$}]{expected\+\_\+rewards\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{expected\+\_\+target\+\_\+densities\+\_\+out, }
\item[{int $\ast$}]{expected\+\_\+target\+\_\+densities\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{comp\+\_\+etd\+\_\+history\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{comp\+\_\+reward\+\_\+history\+\_\+out\+\_\+dim2}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_acafd12e5bc30f51b703a73606d76de40}{}\label{classVIPS__PythonWrapper_acafd12e5bc30f51b703a73606d76de40}
Returns the expected rewards and expected target densities \begin{DoxyReturn}{Returns}
expected\+\_\+rewards\+\_\+out a vector containing for each component the expect reward that was used during the last weight optimization 

expected\+\_\+target\+\_\+densities\+\_\+out a vector containing the W\+IS estimates of E\mbox{[}f(x)\mbox{]} 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+last\+\_\+\+K\+Ls@{get\+\_\+last\+\_\+\+K\+Ls}}
\index{get\+\_\+last\+\_\+\+K\+Ls@{get\+\_\+last\+\_\+\+K\+Ls}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+last\+\_\+\+K\+Ls(double $\ast$\+K\+L\+\_\+weights\+\_\+out, double $\ast$$\ast$kls\+\_\+comp\+\_\+out, int $\ast$kls\+\_\+comp\+\_\+out\+\_\+dim1)}{get_last_KLs(double *KL_weights_out, double **kls_comp_out, int *kls_comp_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+last\+\_\+\+K\+Ls (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{K\+L\+\_\+weights\+\_\+out, }
\item[{double $\ast$$\ast$}]{kls\+\_\+comp\+\_\+out, }
\item[{int $\ast$}]{kls\+\_\+comp\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_aa48468dd40833596d32e202fade3af84}{}\label{classVIPS__PythonWrapper_aa48468dd40833596d32e202fade3af84}
Get the K\+Ls of the last update iteration \begin{DoxyReturn}{Returns}
K\+L\+\_\+weights\+\_\+out, the KL D(p\+\_\+new(o)$\vert$$\vert$p\+\_\+old(new)) after the last weight update 

K\+L\+\_\+comps\+\_\+out, a vector containing the K\+Ls D(p\+\_\+new(x$\vert$o)$\vert$$\vert$p\+\_\+old(x$\vert$o)) for each component o. 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture@{get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture}}
\index{get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture@{get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture(double $\ast$samples\+\_\+ptr, int samples\+\_\+dim1, int samples\+\_\+dim2, double $\ast$$\ast$sample\+\_\+densities\+\_\+out, int $\ast$sample\+\_\+densities\+\_\+out\+\_\+dim1)}{get_log_densities_on_mixture(double *samples_ptr, int samples_dim1, int samples_dim2, double **sample_densities_out, int *sample_densities_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+log\+\_\+densities\+\_\+on\+\_\+mixture (
\begin{DoxyParamCaption}
\item[{double $\ast$}]{samples\+\_\+ptr, }
\item[{int}]{samples\+\_\+dim1, }
\item[{int}]{samples\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{sample\+\_\+densities\+\_\+out, }
\item[{int $\ast$}]{sample\+\_\+densities\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a4cd577f2c0ae57ba85fff0746f7c7d7a}{}\label{classVIPS__PythonWrapper_a4cd577f2c0ae57ba85fff0746f7c7d7a}
Evaluates the samples on the learned \hyperlink{classGMM}{G\+MM} and returns log(p(samples)) 
\begin{DoxyParams}{Parameters}
{\em samples\+\_\+ptr} & -\/ The samples to be evaluated \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
sample\+\_\+densities -\/ the log densities on the \hyperlink{classGMM}{G\+MM} 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+model@{get\+\_\+model}}
\index{get\+\_\+model@{get\+\_\+model}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+model(double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$means\+\_\+out\+\_\+ptr, int $\ast$means\+\_\+out\+\_\+dim1, int $\ast$means\+\_\+out\+\_\+dim2, double $\ast$$\ast$covs\+\_\+out\+\_\+ptr, int $\ast$covs\+\_\+out\+\_\+dim1, int $\ast$covs\+\_\+out\+\_\+dim2, int $\ast$covs\+\_\+out\+\_\+dim3)}{get_model(double **weights_out_ptr, int *weights_out_dim1, double **means_out_ptr, int *means_out_dim1, int *means_out_dim2, double **covs_out_ptr, int *covs_out_dim1, int *covs_out_dim2, int *covs_out_dim3)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+model (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{weights\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{weights\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{means\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{means\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{means\+\_\+out\+\_\+dim2, }
\item[{double $\ast$$\ast$}]{covs\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim1, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim2, }
\item[{int $\ast$}]{covs\+\_\+out\+\_\+dim3}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_acb7c2dea0de7913cc96a8d9511cd96a8}{}\label{classVIPS__PythonWrapper_acb7c2dea0de7913cc96a8d9511cd96a8}
Returns the learned \hyperlink{classGMM}{G\+MM}. \begin{DoxyReturn}{Returns}
weights -\/ weights for each component 

means -\/ means for each component 

covs -\/ covariance matrices for each component 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+model\+\_\+entropies@{get\+\_\+model\+\_\+entropies}}
\index{get\+\_\+model\+\_\+entropies@{get\+\_\+model\+\_\+entropies}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+model\+\_\+entropies(double $\ast$$\ast$entropies\+\_\+out\+\_\+ptr, int $\ast$entropies\+\_\+out\+\_\+dim1)}{get_model_entropies(double **entropies_out_ptr, int *entropies_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+model\+\_\+entropies (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{entropies\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{entropies\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_acaee5a15878205a9ad0ce90c0ffbbdec}{}\label{classVIPS__PythonWrapper_acaee5a15878205a9ad0ce90c0ffbbdec}
Returns the entropies of the learned model \begin{DoxyReturn}{Returns}
entropies\+\_\+out\+\_\+ptr -\/ vector containing the entropy of each component of the learned \hyperlink{classGMM}{G\+MM} 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+num\+\_\+components@{get\+\_\+num\+\_\+components}}
\index{get\+\_\+num\+\_\+components@{get\+\_\+num\+\_\+components}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+num\+\_\+components(int $\ast$num\+\_\+components\+\_\+out)}{get_num_components(int *num_components_out)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+num\+\_\+components (
\begin{DoxyParamCaption}
\item[{int $\ast$}]{num\+\_\+components\+\_\+out}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_aeef466a77710252e595fcd6e9909f975}{}\label{classVIPS__PythonWrapper_aeef466a77710252e595fcd6e9909f975}
Returns the number of components of the \hyperlink{classGMM}{G\+MM} approximation. \begin{DoxyReturn}{Returns}
the number of components 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+num\+\_\+samples@{get\+\_\+num\+\_\+samples}}
\index{get\+\_\+num\+\_\+samples@{get\+\_\+num\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+num\+\_\+samples(int $\ast$num\+\_\+samples, int $\ast$num\+\_\+samples\+\_\+total)}{get_num_samples(int *num_samples, int *num_samples_total)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+num\+\_\+samples (
\begin{DoxyParamCaption}
\item[{int $\ast$}]{num\+\_\+samples, }
\item[{int $\ast$}]{num\+\_\+samples\+\_\+total}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_aa650a92bb89042882d548a16fdf46205}{}\label{classVIPS__PythonWrapper_aa650a92bb89042882d548a16fdf46205}
Gets the number of samples. \begin{DoxyReturn}{Returns}
num\+\_\+samples -\/ the number of samples that are currently activated for learning 

num\+\_\+samples\+\_\+total -\/ the number of samples that have ever been added to the database. 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!get\+\_\+weights@{get\+\_\+weights}}
\index{get\+\_\+weights@{get\+\_\+weights}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{get\+\_\+weights(double $\ast$$\ast$weights\+\_\+out\+\_\+ptr, int $\ast$weights\+\_\+out\+\_\+dim1, double $\ast$$\ast$log\+\_\+weights\+\_\+out\+\_\+ptr, int $\ast$log\+\_\+weights\+\_\+out\+\_\+dim1)}{get_weights(double **weights_out_ptr, int *weights_out_dim1, double **log_weights_out_ptr, int *log_weights_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::get\+\_\+weights (
\begin{DoxyParamCaption}
\item[{double $\ast$$\ast$}]{weights\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{weights\+\_\+out\+\_\+dim1, }
\item[{double $\ast$$\ast$}]{log\+\_\+weights\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{log\+\_\+weights\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a6533e3114818b6dc596a51bb8d8aae32}{}\label{classVIPS__PythonWrapper_a6533e3114818b6dc596a51bb8d8aae32}
Get the \hyperlink{classGMM}{G\+MM} weights \begin{DoxyReturn}{Returns}
weights\+\_\+out\+\_\+ptr, the weights p(o) 

log\+\_\+weights\+\_\+out\+\_\+ptr, the log-\/weights log(p(o)) 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!promote\+\_\+samples\+\_\+to\+\_\+components@{promote\+\_\+samples\+\_\+to\+\_\+components}}
\index{promote\+\_\+samples\+\_\+to\+\_\+components@{promote\+\_\+samples\+\_\+to\+\_\+components}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{promote\+\_\+samples\+\_\+to\+\_\+components(int N, double max\+\_\+exploration\+\_\+bonus, double tau, bool only\+\_\+check\+\_\+active\+\_\+samples, int max\+\_\+samples, bool scale\+\_\+entropy=true, bool isotropic=false)}{promote_samples_to_components(int N, double max_exploration_bonus, double tau, bool only_check_active_samples, int max_samples, bool scale_entropy=true, bool isotropic=false)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::promote\+\_\+samples\+\_\+to\+\_\+components (
\begin{DoxyParamCaption}
\item[{int}]{N, }
\item[{double}]{max\+\_\+exploration\+\_\+bonus, }
\item[{double}]{tau, }
\item[{bool}]{only\+\_\+check\+\_\+active\+\_\+samples, }
\item[{int}]{max\+\_\+samples, }
\item[{bool}]{scale\+\_\+entropy = {\ttfamily true}, }
\item[{bool}]{isotropic = {\ttfamily false}}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a8b17b5e96e7535358411ab8adcbcd934}{}\label{classVIPS__PythonWrapper_a8b17b5e96e7535358411ab8adcbcd934}
Selects promising locations among the current samples and create new components at these positions.~\newline
 The covariance matrices are given by the weighted sums of the covariance matrices and weights of the current model, where the weights are given by the responsibilities of the model components for the new location. Locations are promising if the residual given by residual = log(p\+\_\+intractable(x)) -\/ log(p\+\_\+model(x) + exp(max\+\_\+exploration\+\_\+bonus)) is high.


\begin{DoxyParams}{Parameters}
{\em N} & -\/ the number of samples to be promoted ~\newline
 \\
\hline
{\em max\+\_\+exploration\+\_\+bonus} & -\/ maximum bonus for samples that have low density on the current model ~\newline
 \\
\hline
{\em only\+\_\+check\+\_\+active\+\_\+samples} & -\/ if set to true, the residual is computed on the active samples only, otherwise, the residual is computed for all samples in the sample database. ~\newline
 \\
\hline
{\em tau} & -\/ can be used to assign higher weight to the bonus for samples that have low density on the current model \\
\hline
{\em max\+\_\+samples} & -\/ maximum number of samples to be considered ~\newline
 \\
\hline
{\em scale\+\_\+entropy} & -\/ if true, scale the entropy of the newly created component such that it corresponds to the average entropy  p(o) H(p(x$\vert$o)) \\
\hline
{\em isotropic} & -\/ if true, initialize new components as isotropic (identity-\/matrix if scale\+\_\+entropy = False) instead of interpolating. \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!recompute\+\_\+densities@{recompute\+\_\+densities}}
\index{recompute\+\_\+densities@{recompute\+\_\+densities}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{recompute\+\_\+densities(bool only\+\_\+weights\+\_\+changed)}{recompute_densities(bool only_weights_changed)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::recompute\+\_\+densities (
\begin{DoxyParamCaption}
\item[{bool}]{only\+\_\+weights\+\_\+changed}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_abf63652a9d7589cc189837ba41bcf322}{}\label{classVIPS__PythonWrapper_abf63652a9d7589cc189837ba41bcf322}
Recomputes the densities of various distributions as well as the importance weights. This is usually necessary, after the samples or the model has changed. 
\begin{DoxyParams}{Parameters}
{\em only\+\_\+weights\+\_\+changed} & -\/ if this flag is set to true, assume that only the model weights have changed and avoid recomputing the component densities p(s$\vert$o). \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!restore\+\_\+learner@{restore\+\_\+learner}}
\index{restore\+\_\+learner@{restore\+\_\+learner}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{restore\+\_\+learner()}{restore_learner()}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::restore\+\_\+learner (
\begin{DoxyParamCaption}
{}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a3819321597ef3f522bd577f479ff81b0}{}\label{classVIPS__PythonWrapper_a3819321597ef3f522bd577f479ff81b0}
restores the state of the learner from a backup. \begin{DoxySeeAlso}{See also}
\hyperlink{classVIPS__PythonWrapper_a007140eac0ea0ea69de16a7eb8c13293}{backup\+\_\+learner()} 
\end{DoxySeeAlso}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!select\+\_\+active\+\_\+samples@{select\+\_\+active\+\_\+samples}}
\index{select\+\_\+active\+\_\+samples@{select\+\_\+active\+\_\+samples}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{select\+\_\+active\+\_\+samples(int num\+\_\+comps, int num\+\_\+samples, double temperature, double $\ast$$\ast$num\+\_\+eff\+\_\+out\+\_\+ptr, int $\ast$num\+\_\+eff\+\_\+out\+\_\+dim1)}{select_active_samples(int num_comps, int num_samples, double temperature, double **num_eff_out_ptr, int *num_eff_out_dim1)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::select\+\_\+active\+\_\+samples (
\begin{DoxyParamCaption}
\item[{int}]{num\+\_\+comps, }
\item[{int}]{num\+\_\+samples, }
\item[{double}]{temperature, }
\item[{double $\ast$$\ast$}]{num\+\_\+eff\+\_\+out\+\_\+ptr, }
\item[{int $\ast$}]{num\+\_\+eff\+\_\+out\+\_\+dim1}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_ad93d42f4b18f28193aaba35129e7c068}{}\label{classVIPS__PythonWrapper_ad93d42f4b18f28193aaba35129e7c068}
This function is used for building a mixture model for importance sampling.~\newline
 For each component p(x$\vert$o) in the current approximation, iteratively sample num\+\_\+comps components p(x$\vert$i) from the sample database (these have been used for drawing samples before) according to p  p(mu\+\_\+i$\vert$o) -\/ n\+\_\+usages Adds all samples from each chosen component and stops if num\+\_\+samples are selected ~\newline
 
\begin{DoxyParams}{Parameters}
{\em num\+\_\+comps} & -\/ maximum number of components that should be selected for each component in the approximation ~\newline
 \\
\hline
{\em num\+\_\+samples} & -\/ maximum number of samples that should be aselected for each component in the approximation ~\newline
 \\
\hline
{\em temperature} & -\/ temperature for scaling the probability for sampling components ~\newline
 \\
\hline
\end{DoxyParams}
\begin{DoxyReturn}{Returns}
a vector containing the number of effective samples for each component 
\end{DoxyReturn}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!update\+\_\+components@{update\+\_\+components}}
\index{update\+\_\+components@{update\+\_\+components}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{update\+\_\+components(double tau, double ridge\+\_\+coefficient, double max\+\_\+entropy\+\_\+decrease, double max\+\_\+active\+\_\+samples, bool dont\+\_\+learn\+\_\+correlations, bool dont\+\_\+recompute\+\_\+densities, bool adapt\+\_\+ridge\+\_\+multipliers)}{update_components(double tau, double ridge_coefficient, double max_entropy_decrease, double max_active_samples, bool dont_learn_correlations, bool dont_recompute_densities, bool adapt_ridge_multipliers)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::update\+\_\+components (
\begin{DoxyParamCaption}
\item[{double}]{tau, }
\item[{double}]{ridge\+\_\+coefficient, }
\item[{double}]{max\+\_\+entropy\+\_\+decrease, }
\item[{double}]{max\+\_\+active\+\_\+samples, }
\item[{bool}]{dont\+\_\+learn\+\_\+correlations, }
\item[{bool}]{dont\+\_\+recompute\+\_\+densities, }
\item[{bool}]{adapt\+\_\+ridge\+\_\+multipliers}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a948b081338679c050ed174c6598383d3}{}\label{classVIPS__PythonWrapper_a948b081338679c050ed174c6598383d3}
Updates the components p(s$\vert$o). 
\begin{DoxyParams}{Parameters}
{\em tau} & -\/ entropy coefficient ~\newline
 \\
\hline
{\em ridge\+\_\+coefficient} & -\/ coefficient used for regularization when fitting the quadratic surrogate ~\newline
 \\
\hline
{\em max\+\_\+entropy\+\_\+decrease} & -\/ maximum allowed decrease in entropy for each component ~\newline
 \\
\hline
{\em max\+\_\+active\+\_\+samples} & -\/ size of the subset of samples that should be used for each component update ~\newline
 \\
\hline
{\em dont\+\_\+learn\+\_\+correlations} & -\/ iff true, fit a quadratic surrogate where R is diagonal (experimental) ~\newline
 \\
\hline
{\em dont\+\_\+recompute\+\_\+densities} & -\/ do not recompute the densities after the component update ~\newline
 \\
\hline
{\em adapt\+\_\+ridge\+\_\+multipliers} & -\/ if true, the ridge\+\_\+coefficient will be adapted for each component ~\newline
 \\
\hline
\end{DoxyParams}
\index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds@{update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds}}
\index{update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds@{update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds(bool update\+\_\+weight\+\_\+targets, bool update\+\_\+comp\+\_\+targets)}{update_targets_for_KL_bounds(bool update_weight_targets, bool update_comp_targets)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::update\+\_\+targets\+\_\+for\+\_\+\+K\+L\+\_\+bounds (
\begin{DoxyParamCaption}
\item[{bool}]{update\+\_\+weight\+\_\+targets, }
\item[{bool}]{update\+\_\+comp\+\_\+targets}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a0e0bab32cbad5c82966440fd5d7e91db}{}\label{classVIPS__PythonWrapper_a0e0bab32cbad5c82966440fd5d7e91db}
Sets the current components p(s$\vert$o) and the current weight distribution p(o) as the target for the respective KL bounds. \index{V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}!update\+\_\+weights@{update\+\_\+weights}}
\index{update\+\_\+weights@{update\+\_\+weights}!V\+I\+P\+S\+\_\+\+Python\+Wrapper@{V\+I\+P\+S\+\_\+\+Python\+Wrapper}}
\subsubsection[{\texorpdfstring{update\+\_\+weights(double epsilon, double tau, double max\+\_\+entropy\+\_\+decrease, bool be\+\_\+greedy)}{update_weights(double epsilon, double tau, double max_entropy_decrease, bool be_greedy)}}]{\setlength{\rightskip}{0pt plus 5cm}void V\+I\+P\+S\+\_\+\+Python\+Wrapper\+::update\+\_\+weights (
\begin{DoxyParamCaption}
\item[{double}]{epsilon, }
\item[{double}]{tau, }
\item[{double}]{max\+\_\+entropy\+\_\+decrease, }
\item[{bool}]{be\+\_\+greedy}
\end{DoxyParamCaption}
)}\hypertarget{classVIPS__PythonWrapper_a79046b01fe1129abfd723c4d48ca1d45}{}\label{classVIPS__PythonWrapper_a79046b01fe1129abfd723c4d48ca1d45}
update the component weights p(o) 
\begin{DoxyParams}{Parameters}
{\em epsilon} & -\/ KL bound ~\newline
 \\
\hline
{\em tau} & -\/ entropy coefficient ~\newline
 \\
\hline
{\em max\+\_\+entropy\+\_\+decrease} & -\/ lower bound on entropy ~\newline
 \\
\hline
{\em be\+\_\+greedy} & -\/ if true, don\textquotesingle{}t perform K\+L-\/bound optimization but use the greedy optimal weights \\
\hline
\end{DoxyParams}


The documentation for this class was generated from the following files\+:\begin{DoxyCompactItemize}
\item 
V\+I\+P\+S\+\_\+\+Python\+Wrapper.\+h\item 
V\+I\+P\+S\+\_\+\+Python\+Wrapper.\+cpp\end{DoxyCompactItemize}
